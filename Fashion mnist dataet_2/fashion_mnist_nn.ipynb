{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fashion MNIST Neural Network - From Scratch\n",
        "**Production-grade implementation using only NumPy and Pandas**\n",
        "\n",
        "---\n",
        "\n",
        "## Mission\n",
        "Classify Fashion MNIST clothing items (10 categories) with **90%+ test accuracy** using a deep neural network built entirely from scratch.\n",
        "\n",
        "## Architecture Specifications\n",
        "**Network Structure**\n",
        "```\n",
        "Input Layer:     784 neurons (28×28 flattened images)\n",
        "Hidden Layer 1:  256 neurons + Leaky ReLU (α=0.01) + Dropout(0.3)\n",
        "Hidden Layer 2:  128 neurons + Leaky ReLU (α=0.01) + Dropout(0.3)\n",
        "Hidden Layer 3:  64 neurons + Leaky ReLU (α=0.01) + Dropout(0.3)\n",
        "Hidden Layer 4:  32 neurons + Leaky ReLU (α=0.01) + Dropout(0.3)\n",
        "Output Layer:    10 neurons + Softmax\n",
        "```\n",
        "\n",
        "**Training Configuration**\n",
        "- **Optimizer**: Adam (β₁=0.9, β₂=0.999, lr=0.001, ε=1e-8)\n",
        "- **Loss Function**: Categorical Cross-Entropy + L2 Regularization (λ=0.01)\n",
        "- **Batch Size**: 64\n",
        "- **Weight Initialization**: He Initialization\n",
        "- **Early Stopping**: Patience=15 epochs on dev set accuracy\n",
        "- **Max Epochs**: 150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration dictionary with all hyperparameters\n",
        "CONFIG = {\n",
        "    'architecture': [784, 256, 128, 64, 32, 10],\n",
        "    'learning_rate': 0.001,\n",
        "    'beta1': 0.9,\n",
        "    'beta2': 0.999,\n",
        "    'epsilon': 1e-8,\n",
        "    'lambda_reg': 0.01,\n",
        "    'dropout_rate': 0.3,\n",
        "    'leaky_alpha': 0.01,\n",
        "    'batch_size': 64,\n",
        "    'max_epochs': 150,\n",
        "    'patience': 15,\n",
        "    'seed': 42,\n",
        "    'train_path': 'fashion-mnist_train.csv',  # Adjusting for actual filename found on disk\n",
        "    'test_path': 'fashion-mnist_test.csv'     # Adjusting for actual filename found on disk\n",
        "}\n",
        "\n",
        "CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print('Configuration loaded successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading fashion-mnist_train.csv...\n",
            "Loading fashion-mnist_test.csv...\n",
            "X_train: (784, 48000), Y_train: (10, 48000)\n",
            "X_dev: (784, 12000), Y_dev: (10, 12000)\n",
            "X_test: (784, 10000), Y_test: (10, 10000)\n",
            "Data loaded and verified successfully!\n"
          ]
        }
      ],
      "source": [
        "class DataLoader:\n",
        "    \"\"\"Handles loading, preprocessing, and splitting Fashion MNIST data.\"\"\"\n",
        "    \n",
        "    def __init__(self, train_path: str, test_path: str, seed: int = 42):\n",
        "        self.train_path = train_path\n",
        "        self.test_path = test_path\n",
        "        self.seed = seed\n",
        "    \n",
        "    def load_and_preprocess(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, \n",
        "                                           np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Load data, split, normalize, and one-hot encode.\"\"\"\n",
        "        # Check if files exist\n",
        "        if not os.path.exists(self.train_path) or not os.path.exists(self.test_path):\n",
        "            raise FileNotFoundError(f\"Data files not found. Expected {self.train_path} and {self.test_path}\")\n",
        "\n",
        "        # Load CSVs\n",
        "        print(f\"Loading {self.train_path}...\")\n",
        "        train_df = pd.read_csv(self.train_path)\n",
        "        print(f\"Loading {self.test_path}...\")\n",
        "        test_df = pd.read_csv(self.test_path)\n",
        "        \n",
        "        # Extract labels and pixels\n",
        "        train_labels = train_df['label'].values\n",
        "        train_pixels = train_df.drop('label', axis=1).values.astype(np.float32)\n",
        "        test_labels = test_df['label'].values\n",
        "        test_pixels = test_df.drop('label', axis=1).values.astype(np.float32)\n",
        "        \n",
        "        # Normalize pixels [0, 255] -> [0, 1]\n",
        "        train_pixels = train_pixels / 255.0\n",
        "        test_pixels = test_pixels / 255.0\n",
        "        \n",
        "        # Train/Dev split (80/20)\n",
        "        np.random.seed(self.seed)\n",
        "        n_samples = len(train_labels)\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        split_idx = int(0.8 * n_samples)\n",
        "        \n",
        "        train_idx, dev_idx = indices[:split_idx], indices[split_idx:]\n",
        "        \n",
        "        # Transpose to (features, samples)\n",
        "        X_train = train_pixels[train_idx].T  # (784, 48000)\n",
        "        Y_train = self._one_hot(train_labels[train_idx])  # (10, 48000)\n",
        "        X_dev = train_pixels[dev_idx].T  # (784, 12000)\n",
        "        Y_dev = self._one_hot(train_labels[dev_idx])  # (10, 12000)\n",
        "        X_test = test_pixels.T  # (784, 10000)\n",
        "        Y_test = self._one_hot(test_labels)  # (10, 10000)\n",
        "        \n",
        "        return X_train, Y_train, X_dev, Y_dev, X_test, Y_test\n",
        "    \n",
        "    def _one_hot(self, labels: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Convert labels to one-hot encoding.\"\"\"\n",
        "        n_classes = 10\n",
        "        one_hot = np.zeros((n_classes, len(labels)), dtype=np.float32)\n",
        "        one_hot[labels, np.arange(len(labels))] = 1.0\n",
        "        return one_hot\n",
        "\n",
        "# Load data\n",
        "loader = DataLoader(CONFIG['train_path'], CONFIG['test_path'], CONFIG['seed'])\n",
        "try:\n",
        "    X_train, Y_train, X_dev, Y_dev, X_test, Y_test = loader.load_and_preprocess()\n",
        "    \n",
        "    # Verify shapes\n",
        "    print(f'X_train: {X_train.shape}, Y_train: {Y_train.shape}')\n",
        "    print(f'X_dev: {X_dev.shape}, Y_dev: {Y_dev.shape}')\n",
        "    print(f'X_test: {X_test.shape}, Y_test: {Y_test.shape}')\n",
        "    \n",
        "    assert X_train.shape == (784, 48000)\n",
        "    assert Y_train.shape == (10, 48000)\n",
        "    assert X_dev.shape == (784, 12000)\n",
        "    assert X_test.shape == (784, 10000)\n",
        "    print('Data loaded and verified successfully!')\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    print(\"Please ensure the CSV files are in the same directory as this notebook.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Activation functions defined\n"
          ]
        }
      ],
      "source": [
        "class LeakyReLU:\n",
        "    \"\"\"Leaky ReLU activation with configurable alpha.\"\"\"\n",
        "    \n",
        "    def __init__(self, alpha: float = 0.01):\n",
        "        self.alpha = alpha\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        self.cache = Z\n",
        "        return np.where(Z > 0, Z, self.alpha * Z)\n",
        "    \n",
        "    def backward(self, dA: np.ndarray) -> np.ndarray:\n",
        "        Z = self.cache\n",
        "        dZ = np.where(Z > 0, 1.0, self.alpha)\n",
        "        return dA * dZ\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"Numerically stable softmax activation.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
        "        # Subtract max for numerical stability to prevent overflow\n",
        "        Z_shifted = Z - np.max(Z, axis=0, keepdims=True)\n",
        "        exp_Z = np.exp(Z_shifted)\n",
        "        A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
        "        self.cache = A\n",
        "        return A\n",
        "\n",
        "print('Activation functions defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Layer Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer components defined\n"
          ]
        }
      ],
      "source": [
        "class Dense:\n",
        "    \"\"\"Fully connected layer with He initialization.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_in: int, n_out: int):\n",
        "        # He Initialization\n",
        "        self.W = np.random.randn(n_out, n_in).astype(np.float32) * np.sqrt(2.0 / n_in)\n",
        "        self.b = np.zeros((n_out, 1), dtype=np.float32)\n",
        "        self.cache = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "    \n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        self.cache = X\n",
        "        # Z = W @ X + b\n",
        "        return self.W @ X + self.b\n",
        "    \n",
        "    def backward(self, dZ: np.ndarray, lambda_reg: float = 0.0) -> np.ndarray:\n",
        "        X = self.cache\n",
        "        m = X.shape[1]\n",
        "        \n",
        "        # Calculate gradients\n",
        "        # dW includes L2 regularization term (derivative of (lambda/2)*sum(W^2) is lambda*W)\n",
        "        self.dW = (dZ @ X.T) / m + lambda_reg * self.W\n",
        "        self.db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "        \n",
        "        # Return gradients for previous layer\n",
        "        return self.W.T @ dZ\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"Dropout layer with inverted dropout scaling.\"\"\"\n",
        "    \n",
        "    def __init__(self, rate: float = 0.3):\n",
        "        self.rate = rate\n",
        "        self.mask = None\n",
        "    \n",
        "    def forward(self, A: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        if not training or self.rate == 0:\n",
        "            return A\n",
        "        \n",
        "        # Inverted dropout: Scale during training so no scaling needed at test time\n",
        "        self.mask = (np.random.rand(*A.shape) > self.rate).astype(np.float32) / (1 - self.rate)\n",
        "        return A * self.mask\n",
        "    \n",
        "    def backward(self, dA: np.ndarray) -> np.ndarray:\n",
        "        if self.mask is None:\n",
        "            return dA\n",
        "        return dA * self.mask\n",
        "\n",
        "print('Layer components defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural Network class defined\n"
          ]
        }
      ],
      "source": [
        "class NeuralNetwork:\n",
        "    \"\"\"Deep neural network with Leaky ReLU, Dropout, and Softmax output.\"\"\"\n",
        "    \n",
        "    def __init__(self, architecture: List[int], dropout_rate: float = 0.3, \n",
        "                 leaky_alpha: float = 0.01):\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "        self.dropouts = []\n",
        "        self.softmax = Softmax()\n",
        "        \n",
        "        # Build hidden layers\n",
        "        # Architecture: [784, 256, 128, 64, 32, 10]\n",
        "        for i in range(len(architecture) - 1):\n",
        "            self.layers.append(Dense(architecture[i], architecture[i+1]))\n",
        "            \n",
        "            # Add activation and dropout for all but the last layer\n",
        "            if i < len(architecture) - 2:  \n",
        "                self.activations.append(LeakyReLU(leaky_alpha))\n",
        "                self.dropouts.append(Dropout(dropout_rate))\n",
        "        \n",
        "        self.cache = {}\n",
        "    \n",
        "    def forward(self, X: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        A = X\n",
        "        self.cache['A0'] = A\n",
        "        \n",
        "        # Forward pass through hidden layers\n",
        "        for i, layer in enumerate(self.layers[:-1]):\n",
        "            Z = layer.forward(A)\n",
        "            self.cache[f'Z{i+1}'] = Z\n",
        "            \n",
        "            A = self.activations[i].forward(Z)\n",
        "            self.cache[f'A{i+1}_pre'] = A\n",
        "            \n",
        "            A = self.dropouts[i].forward(A, training)\n",
        "            self.cache[f'A{i+1}'] = A\n",
        "        \n",
        "        # Output layer\n",
        "        Z = self.layers[-1].forward(A)\n",
        "        self.cache['Z_out'] = Z\n",
        "        A = self.softmax.forward(Z)\n",
        "        self.cache['A_out'] = A\n",
        "        return A\n",
        "    \n",
        "    def backward(self, Y: np.ndarray, lambda_reg: float = 0.0) -> None:\n",
        "        A_out = self.cache['A_out']\n",
        "        \n",
        "        # Gradient of Cross-Entropy + Softmax\n",
        "        dZ = A_out - Y\n",
        "        \n",
        "        # Output layer backward\n",
        "        n_layers = len(self.layers)\n",
        "        dA = self.layers[-1].backward(dZ, lambda_reg)\n",
        "        \n",
        "        # Hidden layers backward (in reverse order)\n",
        "        for i in range(n_layers - 2, -1, -1):\n",
        "            dA = self.dropouts[i].backward(dA)\n",
        "            dA = self.activations[i].backward(dA)\n",
        "            dA = self.layers[i].backward(dA, lambda_reg)\n",
        "    \n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        # Predict mode disables dropout\n",
        "        return self.forward(X, training=False)\n",
        "\n",
        "print('Neural Network class defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adam Optimizer defined\n"
          ]
        }
      ],
      "source": [
        "class AdamOptimizer:\n",
        "    \"\"\"Adam optimizer with bias correction.\"\"\"\n",
        "    \n",
        "    def __init__(self, network: NeuralNetwork, lr: float = 0.001,\n",
        "                 beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8):\n",
        "        self.network = network\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.t = 0\n",
        "        \n",
        "        # Initialize moment estimates\n",
        "        self.m_W = [np.zeros_like(layer.W) for layer in network.layers]\n",
        "        self.v_W = [np.zeros_like(layer.W) for layer in network.layers]\n",
        "        self.m_b = [np.zeros_like(layer.b) for layer in network.layers]\n",
        "        self.v_b = [np.zeros_like(layer.b) for layer in network.layers]\n",
        "    \n",
        "    def update(self) -> None:\n",
        "        self.t += 1\n",
        "        \n",
        "        for i, layer in enumerate(self.network.layers):\n",
        "            # Update Weights\n",
        "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * layer.dW\n",
        "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * (layer.dW ** 2)\n",
        "            \n",
        "            # Bias correction\n",
        "            m_hat_W = self.m_W[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat_W = self.v_W[i] / (1 - self.beta2 ** self.t)\n",
        "            \n",
        "            # Parameter update\n",
        "            layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.epsilon)\n",
        "            \n",
        "            # Update Biases\n",
        "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * layer.db\n",
        "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (layer.db ** 2)\n",
        "            \n",
        "            # Bias correction\n",
        "            m_hat_b = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat_b = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
        "            \n",
        "            # Parameter update\n",
        "            layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
        "\n",
        "print('Adam Optimizer defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Loss Functions and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss functions and metrics defined\n"
          ]
        }
      ],
      "source": [
        "def cross_entropy_loss(Y_pred: np.ndarray, Y_true: np.ndarray) -> float:\n",
        "    \"\"\"Categorical cross-entropy loss.\"\"\"\n",
        "    epsilon = 1e-8\n",
        "    # Standard cross-entropy formula\n",
        "    return -np.mean(np.sum(Y_true * np.log(Y_pred + epsilon), axis=0))\n",
        "\n",
        "\n",
        "def l2_regularization(network: NeuralNetwork, lambda_reg: float) -> float:\n",
        "    \"\"\"L2 regularization term (weights only).\"\"\"\n",
        "    reg_sum = sum(np.sum(layer.W ** 2) for layer in network.layers)\n",
        "    return (lambda_reg / 2) * reg_sum\n",
        "\n",
        "\n",
        "def compute_accuracy(Y_pred: np.ndarray, Y_true: np.ndarray) -> float:\n",
        "    \"\"\"Compute classification accuracy.\"\"\"\n",
        "    pred_labels = np.argmax(Y_pred, axis=0)\n",
        "    true_labels = np.argmax(Y_true, axis=0)\n",
        "    return np.mean(pred_labels == true_labels) * 100\n",
        "\n",
        "\n",
        "def confusion_matrix(Y_pred: np.ndarray, Y_true: np.ndarray, n_classes: int = 10) -> np.ndarray:\n",
        "    \"\"\"Compute confusion matrix.\"\"\"\n",
        "    pred_labels = np.argmax(Y_pred, axis=0)\n",
        "    true_labels = np.argmax(Y_true, axis=0)\n",
        "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
        "    for t, p in zip(true_labels, pred_labels):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "\n",
        "def per_class_accuracy(cm: np.ndarray, class_names: List[str]) -> Dict[str, float]:\n",
        "    \"\"\"Compute per-class accuracy from confusion matrix.\"\"\"\n",
        "    accuracies = {}\n",
        "    for i, name in enumerate(class_names):\n",
        "        total = cm[i].sum()\n",
        "        correct = cm[i, i]\n",
        "        accuracies[name] = (correct / total * 100) if total > 0 else 0.0\n",
        "    return accuracies\n",
        "\n",
        "print('Loss functions and metrics defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer class defined\n"
          ]
        }
      ],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Training loop with mini-batches and early stopping.\"\"\"\n",
        "    \n",
        "    def __init__(self, network: NeuralNetwork, optimizer: AdamOptimizer, config: Dict):\n",
        "        self.network = network\n",
        "        self.optimizer = optimizer\n",
        "        self.config = config\n",
        "        self.history = {'loss': [], 'train_acc': [], 'dev_acc': []}\n",
        "        self.best_weights = None\n",
        "        self.best_dev_acc = 0\n",
        "        self.best_epoch = 0\n",
        "    \n",
        "    def _create_batches(self, X: np.ndarray, Y: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "        m = X.shape[1]\n",
        "        batch_size = self.config['batch_size']\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[:, indices]\n",
        "        Y_shuffled = Y[:, indices]\n",
        "        \n",
        "        batches = []\n",
        "        for i in range(0, m, batch_size):\n",
        "            X_batch = X_shuffled[:, i:i+batch_size]\n",
        "            Y_batch = Y_shuffled[:, i:i+batch_size]\n",
        "            batches.append((X_batch, Y_batch))\n",
        "        return batches\n",
        "    \n",
        "    def _save_weights(self):\n",
        "        self.best_weights = [(layer.W.copy(), layer.b.copy()) for layer in self.network.layers]\n",
        "    \n",
        "    def _restore_weights(self):\n",
        "        for i, layer in enumerate(self.network.layers):\n",
        "            layer.W = self.best_weights[i][0].copy()\n",
        "            layer.b = self.best_weights[i][1].copy()\n",
        "    \n",
        "    def train_epoch(self, X: np.ndarray, Y: np.ndarray) -> float:\n",
        "        batches = self._create_batches(X, Y)\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        for X_batch, Y_batch in batches:\n",
        "            # Forward\n",
        "            Y_pred = self.network.forward(X_batch, training=True)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = cross_entropy_loss(Y_pred, Y_batch)\n",
        "            loss += l2_regularization(self.network, self.config['lambda_reg'])\n",
        "            epoch_loss += loss * X_batch.shape[1]\n",
        "            \n",
        "            # Backward\n",
        "            self.network.backward(Y_batch, self.config['lambda_reg'])\n",
        "            \n",
        "            # Update\n",
        "            self.optimizer.update()\n",
        "        \n",
        "        return epoch_loss / X.shape[1]\n",
        "    \n",
        "    def evaluate(self, X: np.ndarray, Y: np.ndarray) -> Tuple[float, float]:\n",
        "        Y_pred = self.network.predict(X)\n",
        "        loss = cross_entropy_loss(Y_pred, Y)\n",
        "        loss += l2_regularization(self.network, self.config['lambda_reg'])\n",
        "        acc = compute_accuracy(Y_pred, Y)\n",
        "        return loss, acc\n",
        "    \n",
        "    def fit(self, X_train: np.ndarray, Y_train: np.ndarray,\n",
        "            X_dev: np.ndarray, Y_dev: np.ndarray) -> Dict:\n",
        "        \n",
        "        patience_counter = 0\n",
        "        \n",
        "        print('=' * 60)\n",
        "        print('TRAINING FASHION MNIST NEURAL NETWORK')\n",
        "        print('=' * 60)\n",
        "        arch_str = ' -> '.join(map(str, self.config['architecture']))\n",
        "        print(f'Architecture: {arch_str}')\n",
        "        print(f'Training samples: {X_train.shape[1]:,}')\n",
        "        print(f'Dev samples: {X_dev.shape[1]:,}')\n",
        "        print()\n",
        "        \n",
        "        for epoch in range(1, self.config['max_epochs'] + 1):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Train\n",
        "            train_loss = self.train_epoch(X_train, Y_train)\n",
        "            \n",
        "            # Evaluate\n",
        "            _, train_acc = self.evaluate(X_train, Y_train)\n",
        "            dev_loss, dev_acc = self.evaluate(X_dev, Y_dev)\n",
        "            \n",
        "            # Record history\n",
        "            self.history['loss'].append(train_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['dev_acc'].append(dev_acc)\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            \n",
        "            print(f'Epoch {epoch:03d}/{self.config[\"max_epochs\"]} | '\n",
        "                  f'Loss: {train_loss:.4f} | Train Acc: {train_acc:.1f}% | '\n",
        "                  f'Dev Acc: {dev_acc:.1f}% | Time: {elapsed:.1f}s')\n",
        "            \n",
        "            # Early stopping check\n",
        "            if dev_acc > self.best_dev_acc:\n",
        "                self.best_dev_acc = dev_acc\n",
        "                self.best_epoch = epoch\n",
        "                self._save_weights()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            \n",
        "            if patience_counter >= self.config['patience']:\n",
        "                print(f'\\nEarly stopping triggered at epoch {epoch}')\n",
        "                break\n",
        "        \n",
        "        print(f'Best dev accuracy: {self.best_dev_acc:.1f}% (epoch {self.best_epoch})')\n",
        "        self._restore_weights()\n",
        "        \n",
        "        return self.history\n",
        "\n",
        "print('Trainer class defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial loss (should be ~2.3): 2.4830\n",
            "============================================================\n",
            "TRAINING FASHION MNIST NEURAL NETWORK\n",
            "============================================================\n",
            "Architecture: 784 -> 256 -> 128 -> 64 -> 32 -> 10\n",
            "Training samples: 48,000\n",
            "Dev samples: 12,000\n",
            "\n",
            "Epoch 001/150 | Loss: 2.4088 | Train Acc: 78.5% | Dev Acc: 78.6% | Time: 18.6s\n",
            "Epoch 002/150 | Loss: 1.1415 | Train Acc: 79.1% | Dev Acc: 78.8% | Time: 17.0s\n",
            "Epoch 003/150 | Loss: 1.0437 | Train Acc: 80.1% | Dev Acc: 79.8% | Time: 15.4s\n",
            "Epoch 004/150 | Loss: 1.0146 | Train Acc: 81.0% | Dev Acc: 80.8% | Time: 14.7s\n",
            "Epoch 005/150 | Loss: 1.0026 | Train Acc: 81.5% | Dev Acc: 81.8% | Time: 14.2s\n",
            "Epoch 006/150 | Loss: 0.9933 | Train Acc: 81.8% | Dev Acc: 81.3% | Time: 15.2s\n",
            "Epoch 007/150 | Loss: 0.9772 | Train Acc: 81.5% | Dev Acc: 81.2% | Time: 15.3s\n",
            "Epoch 008/150 | Loss: 0.9760 | Train Acc: 82.0% | Dev Acc: 81.2% | Time: 15.4s\n",
            "Epoch 009/150 | Loss: 0.9789 | Train Acc: 82.2% | Dev Acc: 82.1% | Time: 15.3s\n",
            "Epoch 010/150 | Loss: 0.9753 | Train Acc: 82.7% | Dev Acc: 82.3% | Time: 16.4s\n",
            "Epoch 011/150 | Loss: 0.9672 | Train Acc: 82.5% | Dev Acc: 82.0% | Time: 16.9s\n",
            "Epoch 012/150 | Loss: 0.9666 | Train Acc: 82.7% | Dev Acc: 82.5% | Time: 18.6s\n",
            "Epoch 013/150 | Loss: 0.9586 | Train Acc: 80.9% | Dev Acc: 80.5% | Time: 15.4s\n",
            "Epoch 014/150 | Loss: 0.9630 | Train Acc: 81.8% | Dev Acc: 81.8% | Time: 15.6s\n",
            "Epoch 015/150 | Loss: 0.9608 | Train Acc: 81.4% | Dev Acc: 80.9% | Time: 18.3s\n",
            "Epoch 016/150 | Loss: 0.9594 | Train Acc: 82.7% | Dev Acc: 82.2% | Time: 18.2s\n",
            "Epoch 017/150 | Loss: 0.9627 | Train Acc: 82.2% | Dev Acc: 82.1% | Time: 17.0s\n",
            "Epoch 018/150 | Loss: 0.9635 | Train Acc: 82.4% | Dev Acc: 81.9% | Time: 16.4s\n",
            "Epoch 019/150 | Loss: 0.9629 | Train Acc: 82.3% | Dev Acc: 82.1% | Time: 15.1s\n",
            "Epoch 020/150 | Loss: 0.9594 | Train Acc: 82.5% | Dev Acc: 82.2% | Time: 16.0s\n",
            "Epoch 021/150 | Loss: 0.9594 | Train Acc: 83.3% | Dev Acc: 83.2% | Time: 17.7s\n",
            "Epoch 022/150 | Loss: 0.9605 | Train Acc: 83.0% | Dev Acc: 82.6% | Time: 15.9s\n",
            "Epoch 023/150 | Loss: 0.9566 | Train Acc: 83.0% | Dev Acc: 82.8% | Time: 15.5s\n",
            "Epoch 024/150 | Loss: 0.9583 | Train Acc: 82.8% | Dev Acc: 82.5% | Time: 15.3s\n",
            "Epoch 025/150 | Loss: 0.9570 | Train Acc: 80.7% | Dev Acc: 80.5% | Time: 15.8s\n",
            "Epoch 026/150 | Loss: 0.9550 | Train Acc: 82.6% | Dev Acc: 82.3% | Time: 15.0s\n",
            "Epoch 027/150 | Loss: 0.9599 | Train Acc: 83.1% | Dev Acc: 82.8% | Time: 16.2s\n",
            "Epoch 028/150 | Loss: 0.9583 | Train Acc: 83.0% | Dev Acc: 82.9% | Time: 18.4s\n",
            "Epoch 029/150 | Loss: 0.9493 | Train Acc: 82.7% | Dev Acc: 82.7% | Time: 18.5s\n",
            "Epoch 030/150 | Loss: 0.9580 | Train Acc: 81.5% | Dev Acc: 81.4% | Time: 27.1s\n",
            "Epoch 031/150 | Loss: 0.9518 | Train Acc: 80.7% | Dev Acc: 80.1% | Time: 31.0s\n",
            "Epoch 032/150 | Loss: 0.9519 | Train Acc: 82.8% | Dev Acc: 82.5% | Time: 21.0s\n",
            "Epoch 033/150 | Loss: 0.9543 | Train Acc: 82.2% | Dev Acc: 82.2% | Time: 27.7s\n",
            "Epoch 034/150 | Loss: 0.9504 | Train Acc: 82.3% | Dev Acc: 81.9% | Time: 22.3s\n",
            "Epoch 035/150 | Loss: 0.9543 | Train Acc: 82.0% | Dev Acc: 81.9% | Time: 23.6s\n",
            "Epoch 036/150 | Loss: 0.9508 | Train Acc: 82.3% | Dev Acc: 82.4% | Time: 26.8s\n",
            "\n",
            "Early stopping triggered at epoch 36\n",
            "Best dev accuracy: 83.2% (epoch 21)\n"
          ]
        }
      ],
      "source": [
        "# Initialize network\n",
        "np.random.seed(CONFIG['seed'])\n",
        "network = NeuralNetwork(\n",
        "    architecture=CONFIG['architecture'],\n",
        "    dropout_rate=CONFIG['dropout_rate'],\n",
        "    leaky_alpha=CONFIG['leaky_alpha']\n",
        ")\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamOptimizer(\n",
        "    network=network,\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    beta1=CONFIG['beta1'],\n",
        "    beta2=CONFIG['beta2'],\n",
        "    epsilon=CONFIG['epsilon']\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(network, optimizer, CONFIG)\n",
        "\n",
        "# Sanity check and start training only if data was loaded\n",
        "if 'X_train' in globals():\n",
        "    # Sanity check: initial loss should be ~2.3 (-log(1/10))\n",
        "    initial_pred = network.predict(X_train[:, :100])\n",
        "    initial_loss = cross_entropy_loss(initial_pred, Y_train[:, :100])\n",
        "    print(f'Initial loss (should be ~2.3): {initial_loss:.4f}')\n",
        "\n",
        "    # Run Training\n",
        "    history = trainer.fit(X_train, Y_train, X_dev, Y_dev)\n",
        "else:\n",
        "    print(\"Skipping training due to missing data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Final Test Evaluation and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FINAL TEST SET EVALUATION\n",
            "============================================================\n",
            "Test Accuracy: 83.2%\n",
            "Test Loss: 0.4866\n",
            "\n",
            "Confusion Matrix:\n",
            "          0    1    2    3    4    5    6    7    8    9\n",
            "  0  [ 828    9   26   67    1    4   50    0   15    0 ]\n",
            "  1  [   1  971   10   14    1    1    2    0    0    0 ]\n",
            "  2  [  16    3  753    7  186    0   28    0    7    0 ]\n",
            "  3  [  35   27   13  852   53    0   17    0    3    0 ]\n",
            "  4  [   1    2   72   25  864    0   34    0    2    0 ]\n",
            "  5  [   1    0    0    1    1  886    0   70   12   29 ]\n",
            "  6  [ 231    4  140   46  157    0  402    0   19    1 ]\n",
            "  7  [   0    0    0    0    0   34    0  918    2   46 ]\n",
            "  8  [   2    0   16    4   11    2   13    6  944    2 ]\n",
            "  9  [   0    0    0    0    0   18    0   74    1  907 ]\n",
            "\n",
            "Per-Class Accuracy:\n",
            "  T-shirt/top : 82.8%\n",
            "  Trouser     : 97.1%\n",
            "  Pullover    : 75.3%\n",
            "  Dress       : 85.2%\n",
            "  Coat        : 86.4%\n",
            "  Sandal      : 88.6%\n",
            "  Shirt       : 40.2%\n",
            "  Sneaker     : 91.8%\n",
            "  Bag         : 94.4%\n",
            "  Ankle boot  : 90.7%\n",
            "\n",
            "Most Confused Pairs (Top 5):\n",
            "  1. T-shirt/top <-> Shirt: 281 mistakes\n",
            "  2. Pullover <-> Coat: 258 mistakes\n",
            "  3. Coat <-> Shirt: 191 mistakes\n",
            "  4. Pullover <-> Shirt: 168 mistakes\n",
            "  5. Sneaker <-> Ankle boot: 120 mistakes\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "if 'X_test' in globals() and trainer.best_weights is not None:\n",
        "    print('=' * 60)\n",
        "    print('FINAL TEST SET EVALUATION')\n",
        "    print('=' * 60)\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    Y_test_pred = network.predict(X_test)\n",
        "    test_loss = cross_entropy_loss(Y_test_pred, Y_test)\n",
        "    test_acc = compute_accuracy(Y_test_pred, Y_test)\n",
        "\n",
        "    print(f'Test Accuracy: {test_acc:.1f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print()\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(Y_test_pred, Y_test)\n",
        "    print('Confusion Matrix:')\n",
        "    header = '      ' + ''.join(f'{i:>5}' for i in range(10))\n",
        "    print(header)\n",
        "    for i in range(10):\n",
        "        row = f'{i:>3}  [' + ''.join(f'{cm[i,j]:>4} ' for j in range(10)) + ']'\n",
        "        print(row)\n",
        "    print()\n",
        "\n",
        "    # Per-class accuracy\n",
        "    class_acc = per_class_accuracy(cm, CLASS_NAMES)\n",
        "    print('Per-Class Accuracy:')\n",
        "    for name, acc in class_acc.items():\n",
        "        print(f'  {name:12s}: {acc:.1f}%')\n",
        "    print()\n",
        "\n",
        "    # Most confused pairs\n",
        "    print('Most Confused Pairs (Top 5):')\n",
        "    confused = []\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            if i != j:\n",
        "                # Total mistakes between i and j\n",
        "                total = cm[i, j] + cm[j, i]\n",
        "                if total > 0:\n",
        "                    confused.append((CLASS_NAMES[i], CLASS_NAMES[j], total))\n",
        "\n",
        "    # Remove duplicates (e.g., A<->B is same as B<->A) and sort\n",
        "    seen = set()\n",
        "    unique_confused = []\n",
        "    for a, b, c in sorted(confused, key=lambda x: -x[2]):\n",
        "        pair = tuple(sorted([a, b]))\n",
        "        if pair not in seen:\n",
        "            seen.add(pair)\n",
        "            unique_confused.append((a, b, c))\n",
        "\n",
        "    for i, (a, b, count) in enumerate(unique_confused[:5], 1):\n",
        "        print(f'  {i}. {a} <-> {b}: {count} mistakes')\n",
        "\n",
        "    print()\n",
        "    print('=' * 60)\n",
        "    print('TRAINING COMPLETE')\n",
        "    print('=' * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **My first run I noticed Per-Class Accuracy:\n",
        "  T-shirt/top : 82.8%\n",
        "  Trouser     : 97.1%\n",
        "  Pullover    : 75.3%\n",
        "  Dress       : 85.2%\n",
        "  Coat        : 86.4%\n",
        "  Sandal      : 88.6%\n",
        "  Shirt       : 40.2%\n",
        "  Sneaker     : 91.8%\n",
        "  Bag         : 94.4%\n",
        "  Ankle boot  : 90.7%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
